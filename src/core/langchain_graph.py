from __future__ import annotations

import os
from typing import Any, Dict

from langchain.graphs import Neo4jGraph
from langchain.chains import GraphCypherQAChain
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.llms.base import BaseLLM

from pydantic import BaseModel


class LangchainGraphConfig(BaseModel):
    """
    Configuration for LangchainGraph.

    Attributes:
        uri: Neo4j connection URI.
        user: Username for Neo4j.
        password: Password for Neo4j.
        model_name: Name of the OpenAI model to use.
        temperature: Sampling temperature for the model.
    """

    uri: str
    user: str
    password: str
    model_name: str = "gpt-3.5-turbo"
    temperature: float = 0.0


class LangchainGraph:
    """
    Provides a retrieval-augmented question answering interface over a Neo4j
    knowledge graph using LangChain's GraphCypherQAChain.
    """

    def __init__(self, config: LangchainGraphConfig) -> None:
        self.config = config
        # Initialize Neo4jGraph wrapper
        self._graph = Neo4jGraph(
            url=self.config.uri,
            username=self.config.user,
            password=self.config.password,
        )
        # Initialize LLM (ChatOpenAI) with specified model and temperature
        self.llm: BaseLLM = ChatOpenAI(
            model_name=self.config.model_name,
            temperature=self.config.temperature,
        )
        # Set up QA chain with a custom prompt that instructs the LLM to
        # generate Cypher queries and answer based on the query results.
        prompt = (
            "You are an AI assistant answering questions using the Neo4j knowledge graph.\n"
            "Given the user's question and the graph schema, generate a Cypher query to retrieve "
            "relevant information. Then provide a concise and factual answer based on the query result."
        )
        self.qa_chain = GraphCypherQAChain.from_llm(
            llm=self.llm,
            graph=self._graph,
            qa_prompt=PromptTemplate(
                input_variables=["schema", "question", "result"],
                template=prompt,
            ),
        )

    @property
    def graph(self) -> Neo4jGraph:
        """Expose the underlying Neo4jGraph instance."""
        return self._graph

    def query(self, question: str) -> str:
        """
        Ask a question using the GraphCypherQAChain.

        Args:
            question: The user's question in natural language.

        Returns:
            The answer string generated by the LLM using graph context.
        """
        response = self.qa_chain.run(question)
        return response
